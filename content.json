{"meta":{"title":"Divenire's Bolg","subtitle":"","description":"","author":"Yue Qiu","url":"http://divenire.ltd","root":"/"},"pages":[{"title":"categories1","date":"2020-11-14T12:03:31.000Z","updated":"2020-11-14T12:06:53.000Z","comments":true,"path":"categories/index.html","permalink":"http://divenire.ltd/categories/index.html","excerpt":"","text":""},{"title":"Page","date":"2013-12-26T14:52:56.000Z","updated":"2020-11-12T08:12:49.000Z","comments":true,"path":"page/index.html","permalink":"http://divenire.ltd/page/index.html","excerpt":"","text":"This is a page test."}],"posts":[{"title":"非线性优化1--高斯牛顿法","slug":"非线性优化1——高斯牛顿法","date":"2020-11-12T07:57:22.000Z","updated":"2020-11-14T09:17:11.000Z","comments":true,"path":"2020/11/12/非线性优化1——高斯牛顿法/","link":"","permalink":"http://divenire.ltd/2020/11/12/%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%961%E2%80%94%E2%80%94%E9%AB%98%E6%96%AF%E7%89%9B%E9%A1%BF%E6%B3%95/","excerpt":"高斯牛顿法的推倒与应用实例。","text":"高斯牛顿法的推倒与应用实例。 非线性优化1——高斯牛顿法 推导 ​ 假设观测到 \\(\\mathrm{N}\\) 个数据点 \\(\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\ldots,\\left(x_{N}, y_{N}\\right)\\right\\},\\) 其中 \\(x \\in \\mathbb{R}^{M}\\) 。希望找到包含 \\(\\mathrm{M}\\) 个参数的非线性函数 \\(f\\left(x, a_{1}, a_{2}, \\ldots, a_{M}\\right),\\) 拟合上述N个数据点。 ​ 为了方便书写, 记: \\(\\quad f_{1}(\\mathbf{a})=f\\left(x_{1}, a_{1}, \\ldots a_{M}\\right)_{\\circ}\\) 则最小二乘的目标函数为: \\[ \\varepsilon(\\mathbf{a})=\\sum_{i=1}^{N}\\left\\|f_{i}(\\mathbf{a})-y_{i}\\right\\|^{2} \\tag{1} \\] ​ 我们需要找到 \\(\\mathbf{a}=\\left[a_{1}, a_{2}, \\ldots, a_{M}\\right]^{T},\\) 使得(1) 的值最小。将 (1) 对 \\(\\mathbf{a}_{j}\\) 求导: \\[ \\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{j}}=\\sum_{i=1}^{N} 2\\left(f_{i}(\\mathbf{a})-y_{i}\\right) \\cdot \\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{j}} \\tag{2} \\] ​ 因此可得\\(\\varepsilon(\\mathbf{a})\\)的梯度为：\\(\\nabla \\varepsilon(\\mathbf{a}) =\\left[\\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{1}}, \\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{2}}, \\ldots, \\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{M}}\\right]^{T}\\). \\[ \\nabla \\varepsilon(\\mathbf{a}) = \\left[\\begin{array}{cccc} \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{1}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{1}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{1}} \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{2}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{2}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{M}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{M}} \\end{array}\\right] \\times \\left[\\begin{array}{cccc} f_{1}(\\mathbf{a})-y_{1} \\\\ f_{2}(\\mathbf{a})-y_{2} \\\\ \\vdots \\\\ f_{N}(\\mathbf{a})-y_{N} \\end{array}\\right] \\] ​ 且有： \\[ \\mathbf{J}_i = \\left[\\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{1}}, \\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{2}}, \\ldots, \\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{M}}\\right]^{T} \\mathbf{r}=\\left[\\begin{array}{c} f_{1}(\\mathbf{a})-y_{1} \\\\ f_{2}(\\mathbf{a})-y_{2} \\\\ \\vdots f_{N}(\\mathbf{a})-y_{N} \\end{array}\\right]\\\\ \\] \\[ \\mathbf{J} = \\left[\\begin{array}{cccc} \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{1}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{1}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{1}} \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{2}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{2}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{M}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{M}} \\end{array}\\right] = \\left[\\mathbf{J}_1, \\mathbf{J}_2, \\ldots, \\mathbf{J}_N\\right]^{T}\\\\ \\] ​ 因此写成向量形式有： \\[ \\nabla \\varepsilon(\\mathbf{a})=2 \\mathbf{J}^{}\\left[\\begin{array}{c} \\left(f_{1}(\\mathbf{a})-y_{1}\\right) \\\\ \\vdots \\\\ \\left(f_{N}(\\mathbf{a})-y_{N}\\right) \\end{array}\\right]=2 \\mathbf{J}^{} \\cdot(\\vec{f}(\\mathbf{a})-\\vec{y}) \\] ​ 根据迭代方法找到\\(\\mathbf{a}_{n+1}\\)使得上式等于0，因此有： \\[ \\begin{aligned} \\nabla \\varepsilon\\left(\\mathbf{a}_{n+1}\\right) &amp;=2 \\mathbf{J}^{}\\left(\\vec{f}\\left(\\mathbf{a}_{n+1}\\right)-\\vec{y}\\right) \\\\ &amp; \\approx 2 \\mathbf{J}^{}\\left(\\vec{f}\\left(\\mathbf{a}_{n}\\right)+\\mathbf{J}^{T} \\cdot\\left(\\mathbf{a}_{n+1}-\\mathbf{a}_{n}\\right)-\\vec{y}\\right)=0 \\end{aligned} \\] ​ 对上式整理可得： \\[ \\mathbf{a}_{n+1}=\\mathbf{a}_{n}-\\left(\\mathbf{JJ^{T}} \\right)^{-1} \\mathbf J \\cdot \\mathbf{r} \\\\ \\underbrace{\\mathbf{JJ^{T}}}_{\\mathbf H}\\Delta \\mathbf{a} = \\underbrace{\\mathbf J \\cdot \\mathbf{r}}_{\\mathbf{g}} \\tag{3} \\] ​ 式(3)即是关于\\(\\Delta a\\)的线性方程组，成为增量方程。对比牛顿法，高斯牛顿法采用\\(\\mathbf{JJ^{T}}\\)作为\\(\\mathbf{H}\\)矩阵的近似，简化了海森矩阵的计算过程。同时将\\(\\mathbf{H}\\)矩阵展开后，可分解为各步\\(\\mathbf{J}_i\\)计算而得： \\[ \\mathbf H =\\mathbf{JJ^{T}} = \\sum_{i=1}^{N}\\mathbf{J}_i\\mathbf{J}_i^T \\] 步骤 给定初始值\\(\\mathbf a_0\\) 对于第k次迭代，求解出当前的雅可比矩阵\\(\\mathbf J\\)以及残差\\(\\mathbf r\\). 根据(3)式，求解增量方程\\(\\Delta \\mathbf a\\) 如果\\(\\Delta \\mathbf a\\)足够小，则停止。否则继续迭代。 实例 图1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 开始Gauss-Newton迭代int iterations = 100; // 迭代次数double cost = 0, lastCost = 0; // 本次迭代的cost和上一次迭代的costfor (int iter = 0; iter &lt; iterations; iter++) &#123; Matrix3d H = Matrix3d::Zero(); // Hessian = J^T J in Gauss-Newton Vector3d b = Vector3d::Zero(); // bias cost = 0; for (int i = 0; i &lt; N; i++) &#123; double xi = x_data[i], yi = y_data[i]; // 第i个数据点 // start your code here double error = 0; // 第i个数据点的计算误差 error = yi - exp(ae * xi * xi + be * xi + ce);; // 填写计算error的表达式 Vector3d J; // 雅可比矩阵 J[0] = -xi * xi * exp(ae * xi * xi + be * xi + ce); // de/da J[1] = -xi * exp(ae * xi * xi + be * xi + ce); // de/db J[2] = -exp(ae * xi * xi + be * xi + ce); // de/dc H += J * J.transpose(); // GN近似的H b += -error * J; // end your code here cost += error * error; &#125; // 求解线性方程 Hx=b，建议用ldlt // start your code here Vector3d dx = H.ldlt().solve(b); // end your code here if (isnan(dx[0])) &#123; cout &lt;&lt; &quot;result is nan!&quot; &lt;&lt; endl; break; &#125; if (iter &gt; 0 &amp;&amp; cost &gt; lastCost) &#123; // 误差增长了，说明近似的不够好 cout &lt;&lt; &quot;cost: &quot; &lt;&lt; cost &lt;&lt; &quot;, last cost: &quot; &lt;&lt; lastCost &lt;&lt; endl; break; &#125; // 更新abc估计值 ae += dx[0]; be += dx[1]; ce += dx[2]; lastCost = cost; cout&lt;&lt;&quot;第&quot;&lt;&lt;iter&lt;&lt;&quot;次迭代结果&quot;&lt;&lt;&quot; a:&quot;&lt;&lt;ae&lt;&lt;&quot; b:&quot;&lt;&lt;be&lt;&lt;&quot; c:&quot;&lt;&lt;ce&lt;&lt;endl; // cout &lt;&lt; &quot;total cost: &quot; &lt;&lt; cost &lt;&lt; endl;&#125;","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"1970-01-01T00:00:00.000Z","updated":"2020-11-12T11:57:43.576Z","comments":true,"path":"1970/01/01/hello-world/","link":"","permalink":"http://divenire.ltd/1970/01/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}