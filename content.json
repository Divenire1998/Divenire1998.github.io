{"meta":{"title":"Divenire's Bolg","subtitle":"This is subtitle","description":"This is Website description","author":"Yue Qiu","url":"https://divenire.ltd","root":"/"},"pages":[{"title":"Page","date":"2013-12-26T22:52:56.000Z","updated":"2020-11-12T08:12:49.000Z","comments":true,"path":"page/index.html","permalink":"https://divenire.ltd/page/index.html","excerpt":"","text":"This is a page test."},{"title":"所有分类","date":"2020-11-18T03:07:02.848Z","updated":"2020-11-18T03:07:02.848Z","comments":true,"path":"categories/index.html","permalink":"https://divenire.ltd/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-11-18T03:06:15.628Z","updated":"2020-11-18T03:06:15.628Z","comments":true,"path":"404.html","permalink":"https://divenire.ltd/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2020-11-18T03:06:43.704Z","updated":"2020-11-18T03:06:43.704Z","comments":true,"path":"about/index.html","permalink":"https://divenire.ltd/about/index.html","excerpt":"","text":"下面写关于自己的内容"},{"title":"我的朋友们","date":"2020-11-18T03:07:40.154Z","updated":"2020-11-18T03:07:40.154Z","comments":true,"path":"friends/index.html","permalink":"https://divenire.ltd/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"所有标签","date":"2020-11-18T03:07:18.406Z","updated":"2020-11-18T03:07:18.406Z","comments":true,"path":"tags/index.html","permalink":"https://divenire.ltd/tags/index.html","excerpt":"","text":""},{"title":"项目","date":"2020-11-18T08:55:04.681Z","updated":"2020-11-18T08:55:04.681Z","comments":true,"path":"project/index.html","permalink":"https://divenire.ltd/project/index.html","excerpt":"","text":""}],"posts":[{"title":"非线性优化1--高斯牛顿法","slug":"非线性优化/非线性优化1——高斯牛顿法","date":"2020-11-12T15:57:22.000Z","updated":"2020-11-18T08:21:15.328Z","comments":true,"path":"2020/11/12/非线性优化/非线性优化1——高斯牛顿法/","link":"","permalink":"https://divenire.ltd/2020/11/12/%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%961%E2%80%94%E2%80%94%E9%AB%98%E6%96%AF%E7%89%9B%E9%A1%BF%E6%B3%95/","excerpt":"高斯牛顿法的推导与应用实例。","text":"高斯牛顿法的推导与应用实例。 推导 ​ 假设观测到 \\(\\mathrm{N}\\) 个数据点 \\(\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\ldots,\\left(x_{N}, y_{N}\\right)\\right\\},\\) 其中 \\(x \\in \\mathbb{R}^{M}\\) 。希望找到包含 \\(\\mathrm{M}\\) 个参数的非线性函数 \\(f\\left(x, a_{1}, a_{2}, \\ldots, a_{M}\\right),\\) 拟合上述N个数据点。 ​ 为了方便书写, 记: \\(\\quad f_{1}(\\mathbf{a})=f\\left(x_{1}, a_{1}, \\ldots a_{M}\\right)_{\\circ}\\) 则最小二乘的目标函数为: \\[ \\varepsilon(\\mathbf{a})=\\sum_{i=1}^{N}\\left\\|f_{i}(\\mathbf{a})-y_{i}\\right\\|^{2} \\tag{1} \\] ​ 我们需要找到 \\(\\mathbf{a}=\\left[a_{1}, a_{2}, \\ldots, a_{M}\\right]^{T},\\) 使得(1) 的值最小。将 (1) 对 \\(\\mathbf{a}_{j}\\) 求导: \\[ \\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{j}}=\\sum_{i=1}^{N} 2\\left(f_{i}(\\mathbf{a})-y_{i}\\right) \\cdot \\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{j}} \\tag{2} \\] ​ 因此可得\\(\\varepsilon(\\mathbf{a})\\)的梯度为：\\(\\nabla \\varepsilon(\\mathbf{a}) =\\left[\\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{1}}, \\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{2}}, \\ldots, \\frac{\\partial \\varepsilon(\\mathbf{a})}{a_{M}}\\right]^{T}\\). \\[ \\nabla \\varepsilon(\\mathbf{a}) = \\left[\\begin{array}{cccc} \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{1}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{1}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{1}} \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{2}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{2}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{M}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{M}} \\end{array}\\right] \\times \\left[\\begin{array}{cccc} f_{1}(\\mathbf{a})-y_{1} \\\\ f_{2}(\\mathbf{a})-y_{2} \\\\ \\vdots \\\\ f_{N}(\\mathbf{a})-y_{N} \\end{array}\\right] \\] ​ 且有： \\[ \\mathbf{J}_i = \\left[\\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{1}}, \\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{2}}, \\ldots, \\frac{\\partial f_{i}(\\mathbf{a})}{\\partial a_{M}}\\right]^{T} \\mathbf{r}=\\left[\\begin{array}{c} f_{1}(\\mathbf{a})-y_{1} \\\\ f_{2}(\\mathbf{a})-y_{2} \\\\ \\vdots f_{N}(\\mathbf{a})-y_{N} \\end{array}\\right]\\\\ \\] \\[ \\mathbf{J} = \\left[\\begin{array}{cccc} \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{1}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{1}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{1}} \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{2}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{2}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{1}(\\mathbf{a})}{\\partial a_{M}} &amp; \\frac{\\partial f_{2}(\\mathbf{a})}{\\partial a_{2}} &amp; \\cdots &amp; \\frac{\\partial f_{N}(\\mathbf{a})}{\\partial a_{M}} \\end{array}\\right] = \\left[\\mathbf{J}_1, \\mathbf{J}_2, \\ldots, \\mathbf{J}_N\\right]^{T}\\\\ \\] ​ 因此写成向量形式有： \\[ \\nabla \\varepsilon(\\mathbf{a})=2 \\mathbf{J}^{}\\left[\\begin{array}{c} \\left(f_{1}(\\mathbf{a})-y_{1}\\right) \\\\ \\vdots \\\\ \\left(f_{N}(\\mathbf{a})-y_{N}\\right) \\end{array}\\right]=2 \\mathbf{J}^{} \\cdot(\\vec{f}(\\mathbf{a})-\\vec{y}) \\] ​ 根据迭代方法找到\\(\\mathbf{a}_{n+1}\\)使得上式等于0，因此有： \\[ \\begin{aligned} \\nabla \\varepsilon\\left(\\mathbf{a}_{n+1}\\right) &amp;=2 \\mathbf{J}^{}\\left(\\vec{f}\\left(\\mathbf{a}_{n+1}\\right)-\\vec{y}\\right) \\\\ &amp; \\approx 2 \\mathbf{J}^{}\\left(\\vec{f}\\left(\\mathbf{a}_{n}\\right)+\\mathbf{J}^{T} \\cdot\\left(\\mathbf{a}_{n+1}-\\mathbf{a}_{n}\\right)-\\vec{y}\\right)=0 \\end{aligned} \\] ​ 对上式整理可得： \\[ \\mathbf{a}_{n+1}=\\mathbf{a}_{n}-\\left(\\mathbf{JJ^{T}} \\right)^{-1} \\mathbf J \\cdot \\mathbf{r} \\\\ \\underbrace{\\mathbf{JJ^{T}}}_{\\mathbf H}\\Delta \\mathbf{a} = \\underbrace{\\mathbf J \\cdot \\mathbf{r}}_{\\mathbf{g}} \\tag{3} \\] ​ 式(3)即是关于\\(\\Delta a\\)的线性方程组，成为增量方程。对比牛顿法，高斯牛顿法采用\\(\\mathbf{JJ^{T}}\\)作为\\(\\mathbf{H}\\)矩阵的近似，简化了海森矩阵的计算过程。同时将\\(\\mathbf{H}\\)矩阵展开后，可分解为各步\\(\\mathbf{J}_i\\)计算而得： \\[ \\mathbf H =\\mathbf{JJ^{T}} = \\sum_{i=1}^{N}\\mathbf{J}_i\\mathbf{J}_i^T \\] 步骤 给定初始值\\(\\mathbf a_0\\) 对于第k次迭代，求解出当前的雅可比矩阵\\(\\mathbf J\\)以及残差\\(\\mathbf r\\). 根据(3)式，求解增量方程\\(\\Delta \\mathbf a\\) 如果\\(\\Delta \\mathbf a\\)足够小，则停止。否则继续迭代。 实例 图1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 开始Gauss-Newton迭代int iterations = 100; // 迭代次数double cost = 0, lastCost = 0; // 本次迭代的cost和上一次迭代的costfor (int iter = 0; iter &lt; iterations; iter++) &#123; Matrix3d H = Matrix3d::Zero(); // Hessian = J^T J in Gauss-Newton Vector3d b = Vector3d::Zero(); // bias cost = 0; for (int i = 0; i &lt; N; i++) &#123; double xi = x_data[i], yi = y_data[i]; // 第i个数据点 // start your code here double error = 0; // 第i个数据点的计算误差 error = yi - exp(ae * xi * xi + be * xi + ce);; // 填写计算error的表达式 Vector3d J; // 雅可比矩阵 J[0] = -xi * xi * exp(ae * xi * xi + be * xi + ce); // de/da J[1] = -xi * exp(ae * xi * xi + be * xi + ce); // de/db J[2] = -exp(ae * xi * xi + be * xi + ce); // de/dc H += J * J.transpose(); // GN近似的H b += -error * J; // end your code here cost += error * error; &#125; // 求解线性方程 Hx=b，建议用ldlt // start your code here Vector3d dx = H.ldlt().solve(b); // end your code here if (isnan(dx[0])) &#123; cout &lt;&lt; &quot;result is nan!&quot; &lt;&lt; endl; break; &#125; if (iter &gt; 0 &amp;&amp; cost &gt; lastCost) &#123; // 误差增长了，说明近似的不够好 cout &lt;&lt; &quot;cost: &quot; &lt;&lt; cost &lt;&lt; &quot;, last cost: &quot; &lt;&lt; lastCost &lt;&lt; endl; break; &#125; // 更新abc估计值 ae += dx[0]; be += dx[1]; ce += dx[2]; lastCost = cost; cout&lt;&lt;&quot;第&quot;&lt;&lt;iter&lt;&lt;&quot;次迭代结果&quot;&lt;&lt;&quot; a:&quot;&lt;&lt;ae&lt;&lt;&quot; b:&quot;&lt;&lt;be&lt;&lt;&quot; c:&quot;&lt;&lt;ce&lt;&lt;endl; // cout &lt;&lt; &quot;total cost: &quot; &lt;&lt; cost &lt;&lt; endl;&#125;","categories":[{"name":"非线性优化","slug":"非线性优化","permalink":"https://divenire.ltd/categories/%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/"}],"tags":[]}],"categories":[{"name":"非线性优化","slug":"非线性优化","permalink":"https://divenire.ltd/categories/%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/"}],"tags":[]}